{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words of the text:  ['it', 'was', 'a', 'very', 'good', 'movie', '.']\n",
      "\n",
      "After using feature extractor function for bigram:  {('it', 'was'): True, ('was', 'a'): True, ('a', 'very'): True, ('very', 'good'): True, ('good', 'movie'): True, ('movie', '.'): True}\n",
      "\n",
      "After cleaning the words of text:  ['good', 'movie']\n",
      "\n",
      "Cleaned words for bigrams:  ['very', 'good', 'movie']\n",
      "\n",
      "Unigram features:  {'good': True, 'movie': True}\n",
      "\n",
      "Bigram features:  {('very', 'good'): True, ('good', 'movie'): True}\n",
      "\n",
      "All features :  {'good': True, 'movie': True, ('very', 'good'): True, ('good', 'movie'): True}\n",
      "\n",
      "Bag of all words:  {'good': True, 'movie': True, ('very', 'good'): True, ('good', 'movie'): True}\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    " \n",
    "stopwords_english = stopwords.words('english')\n",
    " \n",
    "# clean words, i.e. remove stopwords and punctuation\n",
    "def clean_words(words, stopwords_english):\n",
    "    words_clean = []\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords_english and word not in string.punctuation:\n",
    "            words_clean.append(word)    \n",
    "    return words_clean \n",
    " \n",
    "# feature extractor function for unigram\n",
    "def bag_of_words(words):    \n",
    "    words_dictionary = dict([word, True] for word in words)    \n",
    "    return words_dictionary\n",
    " \n",
    "# feature extractor function for ngrams (bigram)\n",
    "def bag_of_ngrams(words, n=2):\n",
    "    words_ng = []\n",
    "    for item in iter(ngrams(words, n)):\n",
    "        words_ng.append(item)\n",
    "    words_dictionary = dict([word, True] for word in words_ng)    \n",
    "    return words_dictionary\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"It was a very good movie.\"\n",
    "words = word_tokenize(text.lower())\n",
    " \n",
    "print (\"Words of the text: \",words)\n",
    "print (\"\\nAfter using feature extractor function for bigram: \",bag_of_ngrams(words))\n",
    "\n",
    "words_clean = clean_words(words, stopwords_english)\n",
    "print (\"\\nAfter cleaning the words of text: \",words_clean)\n",
    "\n",
    "important_words = ['above', 'below', 'off', 'over', 'under', 'more', 'most', 'such', 'no', 'nor',\n",
    "                   'not', 'only', 'so', 'than', 'too', 'very', 'just', 'but','really','non']\n",
    " \n",
    "stopwords_english_for_bigrams = set(stopwords_english) - set(important_words)\n",
    " \n",
    "words_clean_for_bigrams = clean_words(words, stopwords_english_for_bigrams)\n",
    "print (\"\\nCleaned words for bigrams: \",words_clean_for_bigrams)\n",
    "\n",
    "unigram_features = bag_of_words(words_clean)\n",
    "print (\"\\nUnigram features: \",unigram_features)\n",
    "\n",
    "bigram_features = bag_of_ngrams(words_clean_for_bigrams)\n",
    "print (\"\\nBigram features: \",bigram_features)\n",
    "\n",
    "all_features = unigram_features.copy()\n",
    "all_features.update(bigram_features)\n",
    "print (\"\\nAll features : \",all_features)\n",
    "\n",
    "def bag_of_all_words(words, n=2):\n",
    "    words_clean = clean_words(words, stopwords_english)\n",
    "    words_clean_for_bigrams = clean_words(words, stopwords_english_for_bigrams)\n",
    " \n",
    "    unigram_features = bag_of_words(words_clean)\n",
    "    bigram_features = bag_of_ngrams(words_clean_for_bigrams)\n",
    " \n",
    "    all_features = unigram_features.copy()\n",
    "    all_features.update(bigram_features)\n",
    " \n",
    "    return all_features\n",
    " \n",
    "print (\"\\nBag of all words: \",bag_of_all_words(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews \n",
    " \n",
    "pos_reviews = []\n",
    "for fileid in movie_reviews.fileids('pos'):\n",
    "    words = movie_reviews.words(fileid)\n",
    "    pos_reviews.append(words)\n",
    " \n",
    "neg_reviews = []\n",
    "for fileid in movie_reviews.fileids('neg'):\n",
    "    words = movie_reviews.words(fileid)\n",
    "    neg_reviews.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# positive reviews feature set\n",
    "pos_reviews_set = []\n",
    "for words in pos_reviews:\n",
    "    pos_reviews_set.append((bag_of_all_words(words), 'The review for this movie is POSITIVE'))\n",
    " \n",
    "# negative reviews feature set\n",
    "neg_reviews_set = []\n",
    "for words in neg_reviews:\n",
    "    neg_reviews_set.append((bag_of_all_words(words), 'The review for this movie is NEGATIVE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of positive review set: 1000\n",
      "Length of negitive review set: 1000\n",
      "Length of testing set: 250\n",
      "Length of training set: 1750\n"
     ]
    }
   ],
   "source": [
    "print (\"Length of positive review set:\",len(pos_reviews_set)) \n",
    "print (\"Length of negitive review set:\",len(neg_reviews_set)) \n",
    "# radomize pos_reviews_set and neg_reviews_set\n",
    "# doing so will output different accuracy result everytime we run the program\n",
    "from random import shuffle \n",
    "shuffle(pos_reviews_set)\n",
    "shuffle(neg_reviews_set)\n",
    " \n",
    "test_set = pos_reviews_set[:125] + neg_reviews_set[:125]\n",
    "train_set = pos_reviews_set[125:] + neg_reviews_set[125:]\n",
    "\n",
    "print(\"Length of testing set:\",len(test_set))\n",
    "print(\"Length of training set:\",len(train_set))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    " \n",
    "accuracy = classify.accuracy(classifier, test_set)\n",
    "print(\"Accuracy: \",accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The review for this movie is NEGATIVE\n",
      "The review for this movie is NEGATIVE\n",
      "0.8058020534880993\n",
      "0.19419794651190214\n",
      "The review for this movie is POSITIVE\n",
      "The review for this movie is POSITIVE\n",
      "0.0044838034248832915\n",
      "0.9955161965751139\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Negative review \n",
    " \n",
    "custom_review = \"I hated the film. It was a disaster. Poor direction, bad acting.\"\n",
    "custom_review_tokens = word_tokenize(custom_review)\n",
    "custom_review_set = bag_of_all_words(custom_review_tokens)\n",
    "print (classifier.classify(custom_review_set)) \n",
    "\n",
    "# probability result\n",
    "prob_result = classifier.prob_classify(custom_review_set)\n",
    "print (prob_result.max()) \n",
    "print (prob_result.prob(\"The review for this movie is NEGATIVE\")) \n",
    "print (prob_result.prob(\"The review for this movie is POSITIVE\")) \n",
    "\n",
    "# Positive review correctly classified as positive\n",
    "\n",
    "custom_review = \"It was a wonderful and amazing movie. I loved it. it was quite thrilling and interesting.\"\n",
    "custom_review_tokens = word_tokenize(custom_review)\n",
    "custom_review_set = bag_of_all_words(custom_review_tokens)\n",
    "print (classifier.classify(custom_review_set)) \n",
    " \n",
    "# probability result\n",
    "prob_result = classifier.prob_classify(custom_review_set)\n",
    "print (prob_result.max()) \n",
    "print (prob_result.prob(\"The review for this movie is NEGATIVE\")) \n",
    "print (prob_result.prob(\"The review for this movie is POSITIVE\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def clicked():\n",
    "    custom_review = txt.get()\n",
    "    custom_review_tokens = word_tokenize(custom_review)\n",
    "    custom_review_set = bag_of_all_words(custom_review_tokens)\n",
    "    output=classifier.classify(custom_review_set)\n",
    "    l2.configure(text=output)\n",
    "\n",
    "import tkinter\n",
    "from tkinter import *\n",
    "window=tkinter.Tk()\n",
    "window.title(\"TEXT BASED SENTIMENT ANALYSIS-USING BIGRAM\")\n",
    "label=tkinter.Label(window, text=\" Enter your movie review-\", font=(40))\n",
    "label.pack()\n",
    "\n",
    "l2=tkinter.Label(window)\n",
    "l2.place(relx = 0.5, rely = 0.6, anchor = 's') \n",
    "window.geometry('500x300')\n",
    "\n",
    "txt=Entry(window,width=40)\n",
    "txt.place(relx=0.5,rely=0.2,anchor='center')\n",
    "\n",
    "\n",
    "bt= Button(window, text = 'Enter',bg=\"black\",fg=\"white\", command=clicked)\n",
    "bt.place(relx=0.5,rely=0.4,anchor='center')\n",
    "\n",
    "\n",
    "\n",
    "window.mainloop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
